<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/scripts/train_v2.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/scripts/train_v2.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;CONDSAR v2 训练脚本&#10;- VAE Encoder冻结&#10;- SAR VAE Decoder和Condition Encoder可训练&#10;&quot;&quot;&quot;&#10;import argparse&#10;import sys&#10;import logging&#10;from pathlib import Path&#10;from datetime import datetime&#10;&#10;import torch&#10;import torch.nn as nn&#10;import torch.nn.functional as F&#10;from torch.utils.data import DataLoader&#10;import torch.optim as optim&#10;from torch.optim.lr_scheduler import CosineAnnealingLR&#10;&#10;project_root = Path(__file__).parent.parent&#10;sys.path.insert(0, str(project_root))&#10;sys.path.insert(0, str(project_root / &quot;src&quot;))&#10;sys.path.insert(0, str(project_root / &quot;models&quot;))&#10;&#10;from models.condsar_v2 import CondsarModel&#10;from models.dataset_v2 import create_data_loaders&#10;from src.utils.logger import setup_logger&#10;&#10;try:&#10;    import wandb&#10;    HAS_WANDB = True&#10;except ImportError:&#10;    HAS_WANDB = False&#10;&#10;&#10;class CondsarTrainerV2:&#10;    &quot;&quot;&quot;CONDSAR v2 训练器&quot;&quot;&quot;&#10;&#10;    def __init__(self, config: dict):&#10;        self.config = config&#10;        self.device = torch.device(config['device'])&#10;        &#10;        # 日志记录&#10;        self.logger = setup_logger(&#10;            'condsar_trainer_v2',&#10;            log_dir='./outputs/logs'&#10;        )&#10;&#10;        self.logger.info(&quot;=&quot; * 80)&#10;        self.logger.info(&quot; 初始化 CONDSAR v2 训练器&quot;)&#10;        self.logger.info(&quot;=&quot; * 80)&#10;&#10;        # ==================== 模型初始化 ====================&#10;        self.model = CondsarModel(&#10;            pretrained_vae_path=config.get('pretrained_vae', &quot;stabilityai/sd-vae-ft-mse&quot;),&#10;            num_disaster_types=config['num_disaster_types'],&#10;            embedding_dim=config['embedding_dim'],&#10;            num_damage_levels=4,&#10;            device=str(self.device)&#10;        ).to(self.device)&#10;&#10;        # 冻结VAE encoder，其他可训练&#10;        self.model.freeze_vae_encoder()&#10;        &#10;        # 统计参数&#10;        total_params = sum(p.numel() for p in self.model.parameters())&#10;        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)&#10;        &#10;        self.logger.info(f&quot;✅ 模型加载完成&quot;)&#10;        self.logger.info(f&quot;   总参数数: {total_params:,}&quot;)&#10;        self.logger.info(f&quot;   可训练参数: {trainable_params:,}&quot;)&#10;        self.logger.info(f&quot;   冻结参数: {total_params - trainable_params:,}&quot;)&#10;&#10;        # ==================== 优化器和调度器 ====================&#10;        # 只优化可训练的参数&#10;        self.optimizer = optim.AdamW(&#10;            filter(lambda p: p.requires_grad, self.model.parameters()),&#10;            lr=config['learning_rate'],&#10;            weight_decay=config.get('weight_decay', 1e-5)&#10;        )&#10;&#10;        self.scheduler = CosineAnnealingLR(&#10;            self.optimizer,&#10;            T_max=config['num_epochs'],&#10;            eta_min=1e-6&#10;        )&#10;&#10;        # ==================== WandB ====================&#10;        self.use_wandb = config.get('use_wandb', False) and HAS_WANDB&#10;        if self.use_wandb:&#10;            wandb.init(&#10;                project=config.get('wandb_project', 'condsar'),&#10;                name=config.get('wandb_run_name', f&quot;condsar_v2_{datetime.now().strftime('%Y%m%d_%H%M%S')}&quot;),&#10;                config=config,&#10;                mode='offline' if config.get('wandb_offline', False) else 'online'&#10;            )&#10;            self.logger.info(&quot;✅ WandB 已初始化&quot;)&#10;&#10;        # ==================== 输出目录 ====================&#10;        self.output_dir = Path(config.get('output_dir', './outputs'))&#10;        self.output_dir.mkdir(parents=True, exist_ok=True)&#10;        self.checkpoint_dir = self.output_dir / 'checkpoints'&#10;        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)&#10;&#10;    def train_epoch(self, dataloader: DataLoader, epoch: int) -&gt; dict:&#10;        &quot;&quot;&quot;训练一个epoch&quot;&quot;&quot;&#10;        self.model.train()&#10;        &#10;        epoch_stats = {&#10;            'recon_loss': 0.0,&#10;            'diffusion_loss': 0.0,&#10;            'total_loss': 0.0,&#10;            'num_batches': 0&#10;        }&#10;&#10;        self.logger.info(f&quot;\n Epoch {epoch+1}/{self.config['num_epochs']}&quot;)&#10;        &#10;        for batch_idx, batch in enumerate(dataloader):&#10;            # 数据加载&#10;            rgb = batch['rgb_image'].to(self.device)&#10;            sar_gt = batch['sar_image'].to(self.device)&#10;            mask = batch['building_mask'].to(self.device)&#10;            disaster_type = batch['disaster_type'].to(self.device)&#10;            disaster_severity = batch['disaster_severity'].to(self.device)&#10;&#10;            # ==================== 前向传播 ====================&#10;            sar_pred, outputs = self.model(&#10;                rgb_image=rgb,&#10;                mask=mask,&#10;                disaster_type=disaster_type,&#10;                disaster_severity=disaster_severity,&#10;                sar_image_gt=sar_gt&#10;            )&#10;&#10;            # ==================== 损失计算 ====================&#10;            losses = outputs&#10;            total_loss = losses['total_loss']&#10;&#10;            # ==================== 反向传播 ====================&#10;            self.optimizer.zero_grad()&#10;            total_loss.backward()&#10;            torch.nn.utils.clip_grad_norm_(&#10;                filter(lambda p: p.requires_grad, self.model.parameters()),&#10;                max_norm=1.0&#10;            )&#10;            self.optimizer.step()&#10;&#10;            # ==================== 统计 ====================&#10;            epoch_stats['recon_loss'] += losses['recon_loss'].item()&#10;            epoch_stats['diffusion_loss'] += losses['diffusion_loss'].item()&#10;            epoch_stats['total_loss'] += total_loss.item()&#10;            epoch_stats['num_batches'] += 1&#10;&#10;            # ==================== 日志打印 ====================&#10;            if (batch_idx + 1) % self.config.get('log_frequency', 10) == 0:&#10;                avg_total = epoch_stats['total_loss'] / epoch_stats['num_batches']&#10;                self.logger.info(&#10;                    f&quot;  Batch {batch_idx+1:3d}/{len(dataloader):3d} | &quot;&#10;                    f&quot;Loss: {avg_total:.6f} | &quot;&#10;                    f&quot;Recon: {losses['recon_loss']:.6f} | &quot;&#10;                    f&quot;Diff: {losses['diffusion_loss']:.6f}&quot;&#10;                )&#10;&#10;            # ==================== WandB日志 ====================&#10;            if self.use_wandb:&#10;                step = epoch * len(dataloader) + batch_idx&#10;                wandb.log({&#10;                    'train/batch_loss': total_loss.item(),&#10;                    'train/batch_recon_loss': losses['recon_loss'].item(),&#10;                    'train/batch_diffusion_loss': losses['diffusion_loss'].item(),&#10;                }, step=step)&#10;&#10;        # ==================== Epoch统计 ====================&#10;        epoch_stats['recon_loss'] /= epoch_stats['num_batches']&#10;        epoch_stats['diffusion_loss'] /= epoch_stats['num_batches']&#10;        epoch_stats['total_loss'] /= epoch_stats['num_batches']&#10;&#10;        self.logger.info(&#10;            f&quot;✅ Epoch {epoch+1} 完成: &quot;&#10;            f&quot;Loss={epoch_stats['total_loss']:.6f} | &quot;&#10;            f&quot;Recon={epoch_stats['recon_loss']:.6f} | &quot;&#10;            f&quot;Diff={epoch_stats['diffusion_loss']:.6f}&quot;&#10;        )&#10;&#10;        return epoch_stats&#10;&#10;    def validate(self, dataloader: DataLoader, epoch: int) -&gt; dict:&#10;        &quot;&quot;&quot;验证&quot;&quot;&quot;&#10;        self.model.eval()&#10;        &#10;        val_stats = {&#10;            'recon_loss': 0.0,&#10;            'num_batches': 0&#10;        }&#10;&#10;        with torch.no_grad():&#10;            for batch in dataloader:&#10;                rgb = batch['rgb_image'].to(self.device)&#10;                sar_gt = batch['sar_image'].to(self.device)&#10;                mask = batch['building_mask'].to(self.device)&#10;                disaster_type = batch['disaster_type'].to(self.device)&#10;                disaster_severity = batch['disaster_severity'].to(self.device)&#10;&#10;                sar_pred, outputs = self.model(&#10;                    rgb_image=rgb,&#10;                    mask=mask,&#10;                    disaster_type=disaster_type,&#10;                    disaster_severity=disaster_severity,&#10;                    sar_image_gt=sar_gt&#10;                )&#10;&#10;                val_stats['recon_loss'] += outputs['recon_loss'].item()&#10;                val_stats['num_batches'] += 1&#10;&#10;        val_stats['recon_loss'] /= val_stats['num_batches']&#10;&#10;        self.logger.info(f&quot;✅ 验证完成: Loss={val_stats['recon_loss']:.6f}&quot;)&#10;&#10;        # WandB日志&#10;        if self.use_wandb:&#10;            wandb.log({&#10;                'val/recon_loss': val_stats['recon_loss']&#10;            }, step=epoch)&#10;&#10;        return val_stats&#10;&#10;    def save_checkpoint(self, epoch: int, is_best: bool = False):&#10;        &quot;&quot;&quot;保存检查点&quot;&quot;&quot;&#10;        checkpoint = {&#10;            'epoch': epoch,&#10;            'model_state_dict': self.model.state_dict(),&#10;            'optimizer_state_dict': self.optimizer.state_dict(),&#10;            'scheduler_state_dict': self.scheduler.state_dict(),&#10;            'config': self.config&#10;        }&#10;&#10;        # 最后的检查点&#10;        path = self.checkpoint_dir / f'epoch_{epoch}.pt'&#10;        torch.save(checkpoint, path)&#10;&#10;        # 最佳检查点&#10;        if is_best:&#10;            best_path = self.checkpoint_dir / 'best_model.pt'&#10;            torch.save(checkpoint, best_path)&#10;            self.logger.info(f&quot;✅ 保存最佳模型: {best_path}&quot;)&#10;&#10;    def train(self, source_loader: DataLoader, target_loader: DataLoader = None):&#10;        &quot;&quot;&quot;完整训练循环&quot;&quot;&quot;&#10;        self.logger.info(&quot;\n&quot; + &quot;=&quot; * 80)&#10;        self.logger.info(&quot; 开始训练&quot;)&#10;        self.logger.info(&quot;=&quot; * 80 + &quot;\n&quot;)&#10;&#10;        best_loss = float('inf')&#10;&#10;        for epoch in range(self.config['num_epochs']):&#10;            # 训练&#10;            train_stats = self.train_epoch(source_loader, epoch)&#10;&#10;            # 验证&#10;            if target_loader:&#10;                val_stats = self.validate(target_loader, epoch)&#10;                is_best = val_stats['recon_loss'] &lt; best_loss&#10;                best_loss = min(best_loss, val_stats['recon_loss'])&#10;            else:&#10;                is_best = train_stats['total_loss'] &lt; best_loss&#10;                best_loss = min(best_loss, train_stats['total_loss'])&#10;&#10;            # 保存检查点&#10;            if (epoch + 1) % self.config.get('save_frequency', 10) == 0:&#10;                self.save_checkpoint(epoch, is_best)&#10;&#10;            # 调度器更新&#10;            self.scheduler.step()&#10;&#10;        self.logger.info(&quot;\n&quot; + &quot;=&quot; * 80)&#10;        self.logger.info(&quot;✅ 训练完成！&quot;)&#10;        self.logger.info(&quot;=&quot; * 80)&#10;&#10;&#10;def main():&#10;    parser = argparse.ArgumentParser(description='CONDSAR v2 训练脚本')&#10;    &#10;    # 数据配置&#10;    parser.add_argument('--source-dir', type=str, default='./data/source')&#10;    parser.add_argument('--target-dir', type=str, default='./data/target')&#10;    parser.add_argument('--batch-size', type=int, default=4)&#10;    parser.add_argument('--image-size', type=int, default=512)&#10;    &#10;    # 模型配置&#10;    parser.add_argument('--embedding-dim', type=int, default=128)&#10;    parser.add_argument('--num-disaster-types', type=int, default=5)&#10;    parser.add_argument('--pretrained-vae', type=str, default='stabilityai/sd-vae-ft-mse')&#10;    &#10;    # 训练配置&#10;    parser.add_argument('--num-epochs', type=int, default=100)&#10;    parser.add_argument('--learning-rate', type=float, default=1e-4)&#10;    parser.add_argument('--weight-decay', type=float, default=1e-5)&#10;    parser.add_argument('--num-workers', type=int, default=4)&#10;    &#10;    # 输出和日志&#10;    parser.add_argument('--output-dir', type=str, default='./outputs')&#10;    parser.add_argument('--log-frequency', type=int, default=10)&#10;    parser.add_argument('--save-frequency', type=int, default=10)&#10;    parser.add_argument('--use-wandb', action='store_true')&#10;    parser.add_argument('--wandb-offline', action='store_true')&#10;    parser.add_argument('--wandb-project', type=str, default='condsar')&#10;    &#10;    # 设备&#10;    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')&#10;    &#10;    args = parser.parse_args()&#10;    config = vars(args)&#10;&#10;    # ==================== 创建数据加载器 ====================&#10;    source_loader, target_loader = create_data_loaders(&#10;        source_dir=args.source_dir,&#10;        target_dir=args.target_dir,&#10;        batch_size=args.batch_size,&#10;        num_workers=args.num_workers,&#10;        image_size=args.image_size&#10;    )&#10;&#10;    # ==================== 训练 ====================&#10;    trainer = CondsarTrainerV2(config)&#10;    trainer.train(source_loader, target_loader)&#10;&#10;&#10;if __name__ == '__main__':&#10;    main()&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>
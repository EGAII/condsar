# NeDS
https://www.sciencedirect.com/science/article/pii/S0034425725003839?fr=RR-2&ref=pdf_download&rr=994b1e57eebbd52b
![[file-20251027033541801.png]]
1. Conditions:
	- **Damage Object Info**: Generate m_post(Non-Damage, Minor Damage, Major Damage, and Destroyed) from m_pre(0,1) (probabilistically assigned) -> damage embedding.
	- **Disaster Class and Intensity Info(to diffusion time embedding)**: 
		- **Class Info**: Learnable query embeddings (n_types\*d_models). For learning visual features of different disasters.(using learnable embedding is because some disasters might have similar features)
		- **Intensity info**: min \[1., 0., 0., 0.\])  max(\[0., 0., 0., 1.\]

2. 3 steps training:
	1. **Train** ControlNet(Pretrained Stable Diffusion 2.1) on source domain(I-pre,I_post m_pre, m_post)
	2. **Generate** on target domain (I_pre, m_pre)
		1. Ramdomly select disaster type c, sample intensity to get m_post_sim. 
			*Disaster intensity(\[0.1, 0.4, 0.4, 0.1]) to simulate a higher frequency of minor damages and major damages*
		2. Generate on model trained from step1 (I_pre, m_post_sim, c), get I_post_sim.
	3. **Mixed Train** on Mini-batch 2n: n from source domain real sample, n from genered samples.
		- **Real data**: Building localization and damage classfication loss.
		- **Generated data**: Only damage classfication loss.
``` python
# --- Stage A: Train NeDS ---
for I_pre, I_post_gt, m_pre, m_post_gt, c, s in source_loader:
    z_pre = VAE.encode(I_pre)
    z_mask = MaskEncoder([m_pre, m_post_gt])
    e_c = TypeEmbed(c)
    e_s = SeverityEmbed(s)
    e_disaster = fuse(e_c, e_s)
    z_t = add_noise(z_pre, t)
    pred_noise = UNet(z_t, z_mask, e_disaster, t)
    loss = MSE(pred_noise, true_noise)
    loss.backward(); opt_NeDS.step()

# --- Stage B: Generate synthetic target samples ---
for I_pre, m_pre in target_pre_loader:
    c = random_disaster(); s = random_uniform()
    m_post_sim = sample_mask(m_pre)
    e_c, e_s = TypeEmbed(c), SeverityEmbed(s)
    e_disaster = fuse(e_c, e_s)
    I_post_sim = sample_from_diffusion(I_pre, m_pre, m_post_sim, e_disaster)
    save((I_pre, I_post_sim, m_post_sim))

# --- Stage C: Mixed training of ChangeOS ---
for step in range(total_steps):
    src_batch = next(source_loader)
    tgt_batch = next(synthetic_loader)
    loss_src = loc_loss(src_batch) + cls_loss(src_batch)
    loss_tgt = cls_loss(tgt_batch)  # pseudo-labels only
    (loss_src + loss_tgt).backward(); opt.step()
```

# BRIGHT
### Natural disaster
- Volcano Eruption
- Earthquake
- Wildfire
- Storm
- Flood
*Other 2 kinds of man-made disasters are not considered here*


### Data type
- **GSD**: 0.3-1 
- **Pre-event optical Image** and **post-event SAR**
- **Post-event Mask**: Object damage categories(Background (0),Intact (1),Damaged (2),Destroyed (3))  *might not be used in my research*

# Research
### Motivation
1. Phenomena such as smoke after disaster and various weather conditions make it not always possible to obtain clear optical images in the event of a disaster, and SAR images are not affected by such conditions. SAR generation could provide effective assistance for post-disaster rescue.
2. SAR's sensitivity to surface physics gives it a clear advantage in the following tasks: 
	* **Flood monitoring**: Smooth water surface creates specular reflections that appear black on SAR images, making it easy to extract flood extent quickly and accurately, even at night or on cloudy days.
	* **Landslide Identification**: Changes in surface morphology and moisture content can alter radar echoes, aiding in the identification of unstable slopes.
	* **Seismic damage assessment**: Collapsed buildings produce complex scattering signals that are distinctly different from intact buildings and can be used to preliminarily assess building damage rates.

### Method（Need to improved）
add extra info like height map and landcover to inject extra information.
### Model
ControlNet from pretrained SD

### Inference
#### Input
- Pre-event Optical Image
- \*Pre-event object mask

#### Output
- Synthesis Post-event SAR Image

### Training
#### Ground Truth
- Post-event SAR Image

#### Conditions
- Disaster types and intensity
- \*Pre-event object mask
#### Loss
- Location loss
- Disaster class loss

### Improvable
- In inference, customize disaster intensity instead of setting a default value .
- 


### Question need to solve
- We don't have pre-event mask in dataset. And in practice, it is difficult for us to obtain the pre-disaster mask (unless manually marked). 2 Choice:
	1. Use Pre-event Optical Image and Post-event SAR Image only.(==Will the model learn the parts of the RGB image that are most likely to be destroyed?== )
	2. Train a **Damage Probability Estimator** or **Building Segmentator**
		- [xBD: A Dataset for Assessing Building Damage from Satellite Imagery](https://arxiv.org/pdf/1911.09296)
		- [Building damage assessment for rapid disaster response (ChangeOS)](https://www.sciencedirect.com/science/article/pii/S0034425721003564)Also use in **NeDS** to create **binary building mask** for some data without building footprints.
### Might help
- [A review on synthetic aperture radar-based building damage assessment in disasters](https://www.sciencedirect.com/science/article/pii/S0034425720300626)
- [Towards transferable building damage assessment via unsupervised single-temporal change adaptation](https://www.sciencedirect.com/science/article/pii/S0034425724004425?utm_source=chatgpt.com)

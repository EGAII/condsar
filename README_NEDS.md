# CONDSAR NeDS - ControlNet-based Disaster SAR Generation

**Version 2.0 - Refactored based on NeDS Architecture**

Generate post-disaster SAR images from pre-disaster RGB optical images using ControlNet with disaster-aware conditioning.

## ğŸŒŸ Overview

CONDSAR NeDS is a deep learning model for disaster-aware SAR (Synthetic Aperture Radar) image generation, inspired by the [NeDS (Neural Disaster Simulation)](https://www.sciencedirect.com/science/article/pii/S0034425725003839) paper.

### Key Features

- **RGB-to-SAR Generation**: Converts pre-disaster optical images to post-disaster SAR images
- **ControlNet-based**: Built on Stable Diffusion 2.1 ControlNet architecture
- **Disaster-Aware**: Conditioned on disaster type (Volcano, Earthquake, Wildfire, Storm, Flood) and severity
- **Mask Conditioning**: Uses building/damage masks for spatial control
- **NeDS Training Pipeline**: Follows the 3-stage training procedure from NeDS paper

## ğŸ—ï¸ Architecture

```
Input:
â”œâ”€ RGB Pre-disaster Image (B,3,512,512)
â”œâ”€ Building/Damage Mask (B,1,512,512)      [0=bg, 1=intact, 2=damaged, 3=destroyed]
â”œâ”€ Disaster Type (B,)                      [0-4: Volcano/Earthquake/Wildfire/Storm/Flood]
â””â”€ Disaster Severity (B,)                  [0.0-1.0: intensity]

        â†“ [Frozen VAE Encoder]
        
Pre-event RGB â†’ Latent (B,4,64,64)

        â†“ [Add Noise + ControlNet]
        
Mask â†’ ControlNet Embedding (B,320,64,64)
Disaster Type + Severity â†’ Time Embedding (B,1280)

        â†“ [Diffusion + UNet]
        
Predicted Latent (B,4,64,64)

        â†“ [Trainable SAR Decoder]
        
Output: SAR Post-disaster Image (B,1,512,512)
```

### Model Components

1. **Frozen VAE Encoder**: Encodes RGB images to latent space (from SD2.1)
2. **ControlNet**: Processes mask conditioning + disaster embeddings
3. **Disaster Embeddings**: Learnable embeddings for disaster type + severity
4. **Frozen UNet**: Diffusion backbone (from SD2.1)
5. **Trainable SAR Decoder**: Decodes latent to single-channel SAR image

## ğŸ“‚ Project Structure

```
condsar/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ condsar_neds.py              # Main NeDS model
â”‚   â”œâ”€â”€ training_stage_a_neds.py     # Stage A trainer
â”‚   â”œâ”€â”€ training_utils.py            # Dataset & utilities
â”‚   â””â”€â”€ neds.py                      # Original NeDS reference
â”œâ”€â”€ config_neds.yaml                 # Training configuration
â”œâ”€â”€ train_neds.py                    # Main training script
â”œâ”€â”€ generate_metadata_neds.py        # Data preprocessing script
â”œâ”€â”€ data/                            # Your dataset (create this)
â”‚   â”œâ”€â”€ metadata.json               # Generated by generate_metadata_neds.py
â”‚   â”œâ”€â”€ pre/                        # Pre-disaster RGB images
â”‚   â”œâ”€â”€ post/                       # Post-disaster SAR images
â”‚   â””â”€â”€ mask/                       # Building/damage masks
â””â”€â”€ checkpoints/                     # Model checkpoints (auto-created)
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone repository
git clone <your-repo-url>
cd condsar

# Install dependencies
pip install -r requirements.txt

# Required packages:
# - torch >= 2.0
# - diffusers >= 0.21.0
# - transformers
# - accelerate
# - wandb
# - tifffile
# - pillow
# - pyyaml
```

### 2. Prepare Your Data

Organize your data in the following structure:

```
condsar/data/
â”œâ”€â”€ pre/          # Pre-disaster RGB optical images (.jpg, .png, .tif)
â”œâ”€â”€ post/         # Post-disaster SAR images (.tif recommended)
â””â”€â”€ mask/         # Building/damage masks (.tif, values 0-3)
```

**Disaster Type Naming Convention** (optional):
- Include disaster type in filename: `volcano_image001.jpg`, `earthquake_image002.jpg`
- Or organize in subdirectories: `pre/volcano/`, `pre/earthquake/`, etc.

**Severity Naming Convention** (optional):
- Include severity in filename: `volcano_0.5_image001.jpg` (severity=0.5)
- Or use keywords: `volcano_high_image001.jpg` (severity=0.75)

### 3. Generate Metadata

```bash
python generate_metadata_neds.py \
    --data_dir ./condsar/data \
    --output metadata.json
```

This will:
- Scan `pre/`, `post/`, `mask/` directories
- Match files by name (stem matching)
- Extract disaster type and severity from filenames/directories
- Create `data/metadata.json`

**metadata.json format:**
```json
{
  "image001": {
    "pre": "pre/volcano_image001.jpg",
    "post": "post/volcano_image001.tif",
    "mask": "mask/volcano_image001.tif",
    "disaster_type": 0,
    "severity": 0.75
  }
}
```

### 4. Configure Training

Edit `config_neds.yaml`:

```yaml
data:
  source_dir: "./condsar/data"       # Your data directory
  batch_size: 4                      # Adjust based on GPU memory
  num_disaster_types: 5              # 5 disaster types

training:
  stage_a:
    num_epochs: 100
    learning_rate: 1.0e-4
    use_mixed_precision: true        # FP16 for faster training

hardware:
  device: "cuda:0"                   # or "cuda:1", "cpu"
```

### 5. Train Stage A

```bash
# Basic training
python train_neds.py --config config_neds.yaml --stage a

# With custom settings
python train_neds.py \
    --config config_neds.yaml \
    --stage a \
    --batch_size 8 \
    --num_epochs 200 \
    --device cuda:0

# Resume from checkpoint
python train_neds.py \
    --config config_neds.yaml \
    --stage a \
    --resume checkpoints/stage_a_neds/checkpoint_epoch_50.pt

# Quick test (no W&B, fewer epochs)
python train_neds.py \
    --config config_neds.yaml \
    --stage a \
    --no_wandb \
    --num_epochs 5
```

## ğŸ“Š Training Details

### Stage A: Train on Source Domain

Following NeDS pseudo-code:

```python
for I_pre, I_post_sar, mask, disaster_type, severity in source_loader:
    # 1. Encode pre-event RGB via frozen VAE
    pre_latents = VAE.encode(I_pre)
    
    # 2. Add noise (diffusion forward)
    noise = random_normal()
    t = random_timestep()
    noisy_latents = add_noise(pre_latents, noise, t)
    
    # 3. ControlNet forward with mask + disaster embeddings
    down_samples, mid_sample = ControlNet(
        noisy_latents, t, 
        pre_latents, mask,
        disaster_type, severity
    )
    
    # 4. UNet prediction
    noise_pred = UNet(noisy_latents, t, down_samples, mid_sample)
    
    # 5. Compute loss
    loss = MSE(noise_pred, noise) + 0.1 * L1(SAR_decode(pred), I_post_sar)
    
    # 6. Backprop (only ControlNet + SAR decoder)
    loss.backward()
    optimizer.step()
```

### Loss Components

1. **Diffusion Loss** (weight=1.0): MSE between predicted noise and true noise
2. **Reconstruction Loss** (weight=0.1): L1 loss between predicted SAR and ground truth SAR

### Trainable Components

- âœ… ControlNet (mask embedding, disaster embeddings, fusion layers)
- âœ… SAR VAE Decoder (latent to SAR image)
- âŒ VAE Encoder (frozen, from SD2.1)
- âŒ UNet (frozen, from SD2.1)

### GPU Memory Requirements

| Batch Size | Mixed Precision | GPU Memory |
|------------|----------------|------------|
| 2          | FP16           | ~12 GB     |
| 4          | FP16           | ~20 GB     |
| 8          | FP16           | ~40 GB     |
| 4          | FP32           | ~40 GB     |

**Tips for limited GPU memory:**
- Reduce batch size: `--batch_size 2`
- Use gradient accumulation: Set `gradient_accumulation_steps: 2` in config
- Enable gradient checkpointing: Set `gradient_checkpointing: true` in config

## ğŸ“ˆ Monitoring Training

### Weights & Biases

Training automatically logs to W&B if enabled:

```bash
# Login to W&B (first time only)
wandb login

# Training will create project "condsar_neds"
```

Tracked metrics:
- `train/loss`: Total loss
- `train/diffusion_loss`: Diffusion loss
- `train/reconstruction_loss`: SAR reconstruction loss
- `train/lr`: Learning rate

### Local Logs

Logs saved to `./logs/`:
- `condsar_neds_YYYYMMDD_HHMMSS.log`: Training logs
- Console output with progress bars

### Checkpoints

Saved to `./checkpoints/stage_a_neds/`:
- `checkpoint_epoch_N.pt`: Every N epochs (default: 10)
- `final_model.pt`: Final trained model

Checkpoint contents:
```python
{
    'epoch': int,
    'global_step': int,
    'controlnet_state_dict': OrderedDict,
    'sar_decoder_state_dict': OrderedDict,
    'optimizer_state_dict': OrderedDict,
    'lr_scheduler_state_dict': OrderedDict,
    'scaler_state_dict': OrderedDict  # if mixed precision
}
```

## ğŸ”§ Configuration Reference

### Data Configuration

```yaml
data:
  source_dir: "./condsar/data"       # Dataset path
  image_size: 512                    # Image size (fixed at 512)
  batch_size: 4                      # Batch size
  num_workers: 4                     # DataLoader workers
  num_disaster_types: 5              # Number of disaster types
```

### Model Configuration

```yaml
model:
  pretrained_model_name: "stabilityai/stable-diffusion-2-1-base"
  
  controlnet:
    conditioning_channels: 1         # Mask channels
    num_disaster_types: 5
  
  vae:
    freeze_encoder: true             # Must be true
    freeze_decoder: true
  
  sar_decoder:
    trainable: true                  # Must be true
```

### Training Configuration

```yaml
training:
  stage_a:
    num_epochs: 100
    learning_rate: 1.0e-4
    optimizer: "adamw"
    lr_scheduler: "cosine"
    warmup_steps: 500
    use_mixed_precision: true
    gradient_accumulation_steps: 1
    save_every_n_epochs: 10
```

## ğŸ§ª Troubleshooting

### Common Issues

**1. `metadata.json not found`**
```bash
# Generate metadata first
python generate_metadata_neds.py --data_dir ./condsar/data
```

**2. `No matching files found`**
- Check that `pre/`, `post/`, `mask/` directories exist
- Ensure filenames match (same stem): `image001.jpg`, `image001.tif`
- Check file extensions are supported (jpg, png, tif, etc.)

**3. CUDA Out of Memory**
```yaml
# In config_neds.yaml, reduce batch size
data:
  batch_size: 2  # or 1

# Or enable gradient accumulation
training:
  stage_a:
    gradient_accumulation_steps: 4  # effective batch = 2 * 4 = 8
```

**4. Slow training**
- Enable mixed precision: `use_mixed_precision: true`
- Increase `num_workers` in config
- Use SSD for dataset storage

**5. Cannot extract disaster type**
```python
# Add disaster type to filename
# volcano_image001.jpg
# earthquake_image002.jpg

# Or organize in subdirectories
# pre/volcano/image001.jpg
# pre/earthquake/image002.jpg
```

## ğŸ“š Dataset Requirements

### Image Specifications

| Type | Format | Size | Channels | Value Range |
|------|--------|------|----------|-------------|
| Pre (RGB) | JPG/PNG/TIF | 512x512 | 3 | [0, 255] |
| Post (SAR) | TIF (preferred) | 512x512 | 1 | [0, 255] |
| Mask | TIF/PNG | 512x512 | 1 | {0, 1, 2, 3} |

### Mask Values

- **0**: Background (non-building area)
- **1**: Intact building
- **2**: Damaged building
- **3**: Destroyed building

### Disaster Types

| Index | Type | Description |
|-------|------|-------------|
| 0 | Volcano | Volcanic eruption |
| 1 | Earthquake | Seismic activity |
| 2 | Wildfire | Forest/brush fire |
| 3 | Storm | Hurricane/typhoon |
| 4 | Flood | Flooding event |

### Severity Levels

- **0.0 - 0.25**: Minor/Low
- **0.25 - 0.50**: Moderate
- **0.50 - 0.75**: Major/High
- **0.75 - 1.00**: Severe/Extreme

## ğŸ”¬ Advanced Usage

### Custom Disaster Types

To add more disaster types:

1. Update `num_disaster_types` in config
2. Modify `DISASTER_TYPES` in `generate_metadata_neds.py`
3. Update model initialization

### Data Augmentation

Enable in `config_neds.yaml`:

```yaml
augmentation:
  enabled: true
  random_flip: true
  aug_prob: 0.5
```

### Multi-GPU Training

(Future implementation)

```yaml
hardware:
  num_gpus: 2
  # Uses DataParallel or DistributedDataParallel
```

## ğŸ“– References

- **NeDS Paper**: [Neural Disaster Simulation for Satellite Imagery](https://www.sciencedirect.com/science/article/pii/S0034425725003839)
- **Stable Diffusion**: [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
- **ControlNet**: [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543)

## ğŸ“ Citation

```bibtex
@article{zheng2025neds,
  title={Towards transferable building damage assessment via unsupervised single-temporal change adaptation},
  author={Zheng, Zhuo and others},
  journal={Remote Sensing of Environment},
  year={2025}
}
```

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please open an issue or PR.

## ğŸ“§ Contact

For questions or issues, please open a GitHub issue.

---

**Happy Training! ğŸš€**


2025-11-17 08:02:23 - condsar_trainer - [32mINFO[0m - ================================================================================
2025-11-17 08:02:23 - condsar_trainer - [32mINFO[0m - ================================================================================
2025-11-17 08:02:23 - condsar_trainer - [32mINFO[0m - Loading source dataset from ./data
2025-11-17 08:02:23 - condsar_trainer - [32mINFO[0m - Loaded 3155 images from metadata.json
2025-11-17 08:02:23 - condsar_trainer - [32mINFO[0m - Disaster types: ['Volcano', 'Earthquake', 'Wildfire', 'Flood']
2025-11-17 08:02:23 - condsar_trainer - [32mINFO[0m -    Batch size: 2
2025-11-17 08:02:23 - condsar_trainer - [32mINFO[0m -    Total batches: 1578
2025-11-17 08:02:23 - condsar_trainer - [32mINFO[0m - Creating EnhancedDisasterControlNet...
2025-11-17 08:02:25 - condsar_trainer - [32mINFO[0m - Creating SAR VAE Decoder...
2025-11-17 08:02:25 - condsar_trainer - [32mINFO[0m - 
================================================================================
2025-11-17 08:02:25 - condsar_trainer - [32mINFO[0m - ================================================================================

2025-11-17 08:02:26 - condsar_trainer - [31mERROR[0m - Error in batch 0: Given transposed=1, weight of size [4, 512, 4, 4], expected input[2, 320, 512, 512] to have 4 channels, but got 320 channels instead
2025-11-17 08:02:47 - condsar_trainer - [31mERROR[0m - Error in batch 1: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 719.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-17 08:02:57 - condsar_trainer - [31mERROR[0m - Error in batch 2: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 719.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-17 08:03:10 - condsar_trainer - [31mERROR[0m - Error in batch 3: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 718.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-17 08:03:23 - condsar_trainer - [31mERROR[0m - Error in batch 4: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 719.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-17 08:03:36 - condsar_trainer - [31mERROR[0m - Error in batch 5: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 718.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-17 08:03:49 - condsar_trainer - [31mERROR[0m - Error in batch 6: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 719.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-17 08:04:02 - condsar_trainer - [31mERROR[0m - Error in batch 7: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 718.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-17 08:04:28 - condsar_trainer - [31mERROR[0m - Error in batch 8: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 719.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-17 08:04:28 - condsar_trainer - [31mERROR[0m - Error in batch 9: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 718.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-17 08:04:42 - condsar_trainer - [31mERROR[0m - Error in batch 10: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 719.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-17 08:04:55 - condsar_trainer - [31mERROR[0m - Error in batch 11: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 718.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-17 08:05:08 - condsar_trainer - [31mERROR[0m - Error in batch 12: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 719.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-17 08:05:21 - condsar_trainer - [31mERROR[0m - Error in batch 13: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 718.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-17 08:05:37 - condsar_trainer - [31mERROR[0m - Error in batch 14: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 15.83 GiB is allocated by PyTorch, and 719.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

"""
å®Œæ•´çš„CONDSARè®­ç»ƒè„šæœ¬
æ”¯æŒStage A/B/Cä¸‰é˜¶æ®µè®­ç»ƒï¼Œé›†æˆWandBå¯è§†åŒ–
æ”¯æŒä»YAML/JSONé…ç½®æ–‡ä»¶åŠ è½½å‚æ•°
"""
import argparse
import os
import sys
import json
import yaml
import logging
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))
sys.path.insert(0, str(project_root / "models"))

from src.utils.wandb_utils import WandBVisualizer, VisualizationCallback
from src.utils.logger import setup_logger
from models.training_utils import DisasterSARDataset, MetricsTracker
from models.enhanced_condsar import EnhancedDisasterControlNet, SARVAEDecoder

logger = logging.getLogger(__name__)


# ============================================================================
# é…ç½®åŠ è½½å‡½æ•°
# ============================================================================

def load_config_file(config_path: str) -> Dict[str, Any]:
    """
    ä»YAMLæˆ–JSONæ–‡ä»¶åŠ è½½é…ç½®

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ (.yaml æˆ– .json)

    Returns:
        é…ç½®å­—å…¸
    """
    config_path = Path(config_path)

    if not config_path.exists():
        logger.warning(f"âŒ Config file not found: {config_path}")
        return {}

    try:
        if config_path.suffix in ['.yaml', '.yml']:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"âœ… Loaded YAML config from {config_path}")

        elif config_path.suffix == '.json':
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"âœ… Loaded JSON config from {config_path}")

        else:
            logger.error(f"âŒ Unsupported config format: {config_path.suffix}")
            return {}

        return config
    except Exception as e:
        logger.error(f"âŒ Failed to load config: {e}")
        return {}


def merge_config_with_args(config: Dict, args: argparse.Namespace) -> Dict:
    """
    å°†å‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶åˆå¹¶
    å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§ > é…ç½®æ–‡ä»¶ä¼˜å…ˆçº§

    Args:
        config: ä»é…ç½®æ–‡ä»¶åŠ è½½çš„é…ç½®
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        åˆå¹¶åçš„é…ç½®å­—å…¸
    """
    # æå–é…ç½®æ–‡ä»¶ä¸­çš„è®­ç»ƒå‚æ•°
    if not config:
        return vars(args)

    # è·å–é˜¶æ®µç‰¹å®šçš„é…ç½®
    stage = getattr(args, 'stage', args.stage if 'stage' in vars(args) else 'a')

    # æ„å»ºé…ç½®å­—å…¸
    merged = {}

    # ä¼˜å…ˆçº§ 1: ä»é…ç½®æ–‡ä»¶ä¸­æå–
    if 'training' in config and f'stage_{stage}' in config['training']:
        stage_config = config['training'][f'stage_{stage}']
        for key, value in stage_config.items():
            merged[key] = value

    # ä¼˜å…ˆçº§ 2: ä»æ•°æ®é…ç½®ä¸­æå–
    if 'data' in config:
        for key, value in config['data'].items():
            if key not in merged:
                merged[key] = value

    # ä¼˜å…ˆçº§ 3: ä»æ¨¡å‹é…ç½®ä¸­æå–
    if 'model' in config:
        for key, value in config['model'].items():
            if key not in merged:
                merged[key] = value

    # ä¼˜å…ˆçº§ 4: ä»W&Bé…ç½®ä¸­æå–
    if 'wandb' in config:
        if 'use_wandb' not in merged:
            merged['use_wandb'] = config['wandb'].get('enabled', True)

    # ä¼˜å…ˆçº§ 5: ä»è®¾å¤‡é…ç½®ä¸­æå–
    if 'device' in config:
        if 'device' not in merged:
            merged['device'] = config['device'].get('type', 'cuda')

    # ä¼˜å…ˆçº§ 6: ä»è¾“å‡ºé…ç½®ä¸­æå–
    if 'output' in config:
        if 'output_dir' not in merged:
            merged['output_dir'] = config['output'].get('directory', './outputs')

    # ä¼˜å…ˆçº§æœ€é«˜: å‘½ä»¤è¡Œå‚æ•°è¦†ç›–æ‰€æœ‰
    args_dict = vars(args)
    for key, value in args_dict.items():
        if value is not None:  # åªè¦†ç›–æ˜¾å¼æŒ‡å®šçš„å‚æ•°
            merged[key] = value

    # ç¡®ä¿deviceå’Œå…¶ä»–å…³é”®å‚æ•°æœ‰é»˜è®¤å€¼
    if 'device' not in merged or merged.get('device') is None:
        merged['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'

    logger.info("âœ… Config merged successfully")
    return merged


class TrainingConfig:
    """è®­ç»ƒé…ç½®"""

    def __init__(self, **kwargs):
        # åŸºç¡€é…ç½®
        self.project_name = kwargs.get('project_name', 'condsar')
        self.run_name = kwargs.get('run_name', f"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        self.stage = kwargs.get('stage', 'a')  # a, b, or c

        # Device - with extra safety check
        device_val = kwargs.get('device', None)
        if device_val is None or device_val == 'None':
            device_val = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.device = device_val

        # æ•°æ®é…ç½®
        self.source_dir = kwargs.get('source_dir') or './data'
        self.target_dir = kwargs.get('target_dir') or './data'
        self.image_size = kwargs.get('image_size') or 512

        # æ¨¡å‹é…ç½®
        self.model_channels = kwargs.get('model_channels') or 320
        self.num_disaster_types = kwargs.get('num_disaster_types') or 5
        self.embedding_dim = kwargs.get('embedding_dim') or 128

        # è®­ç»ƒé…ç½®
        self.batch_size = kwargs.get('batch_size') or 4
        self.num_epochs = kwargs.get('num_epochs') or 100
        self.learning_rate = kwargs.get('learning_rate') or 1e-4
        self.weight_decay = kwargs.get('weight_decay') or 1e-5
        self.gradient_accumulation_steps = kwargs.get('gradient_accumulation_steps') or 1

        # ä¼˜åŒ–å™¨é…ç½®
        self.warmup_steps = kwargs.get('warmup_steps') or 1000
        self.use_mixed_precision = kwargs.get('use_mixed_precision', True)

        # æ£€æŸ¥ç‚¹é…ç½®
        self.checkpoint_dir = kwargs.get('checkpoint_dir') or './outputs/checkpoints'
        self.save_frequency = kwargs.get('save_frequency') or 10

        # WandBé…ç½®
        self.use_wandb = kwargs.get('use_wandb', True)
        self.wandb_offline = kwargs.get('wandb_offline', False)
        self.log_frequency = kwargs.get('log_frequency') or 100

        # å¯è§†åŒ–é…ç½®
        self.visualize_features = kwargs.get('visualize_features', True)
        self.visualize_frequency = kwargs.get('visualize_frequency') or 500
        self.output_dir = kwargs.get('output_dir') or './outputs'

    def to_dict(self) -> Dict:
        """è½¬ä¸ºå­—å…¸"""
        return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}

    def save(self, path: str):
        """ä¿å­˜é…ç½®"""
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        with open(path, 'w') as f:
            json.dump(self.to_dict(), f, indent=2)
        logger.info(f"Configuration saved to {path}")


class CondsarTrainer:
    """CONDSARè®­ç»ƒå™¨ - æ”¯æŒStage A/B/C"""

    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = torch.device(config.device)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(config.checkpoint_dir).mkdir(parents=True, exist_ok=True)
        Path(config.output_dir).mkdir(parents=True, exist_ok=True)

        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logger(
            name='condsar_trainer',
            log_dir=f"{config.output_dir}/logs"
        )

        # WandBåˆå§‹åŒ–
        self.visualizer = None
        if config.use_wandb:
            try:
                import wandb
                if config.wandb_offline:
                    os.environ["WANDB_MODE"] = "offline"
                self.visualizer = WandBVisualizer(
                    project_name=config.project_name,
                    run_name=config.run_name
                )
                self.logger.info("âœ… WandB initialized")
            except ImportError:
                self.logger.warning("âš ï¸ WandB not installed, skipping visualization")

        # æ¨¡å‹å’Œä¼˜åŒ–å™¨
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.metrics = MetricsTracker(self.logger)

        # ä¿å­˜é…ç½®
        config.save(f"{config.output_dir}/config_{config.stage}.json")

    def setup_stage_a(self):
        """è®¾ç½®Stage Aè®­ç»ƒ"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸ¯ Setting up STAGE A: Source Domain Training")
        self.logger.info("=" * 80)

        # åŠ è½½æ•°æ®
        self.logger.info(f"Loading source dataset from {self.config.source_dir}")
        self.train_dataset = DisasterSARDataset(
            dataset_dir=self.config.source_dir,
            image_size=self.config.image_size,
            return_mask=True,
            return_metadata=True,
            logger=self.logger
        )

        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=0
        )

        self.logger.info(f"âœ… Loaded {len(self.train_dataset)} training samples")
        self.logger.info(f"   Batch size: {self.config.batch_size}")
        self.logger.info(f"   Total batches: {len(self.train_loader)}")

        # åˆ›å»ºæ¨¡å‹
        self.logger.info("Creating EnhancedDisasterControlNet...")
        self.model = EnhancedDisasterControlNet(
            num_disaster_types=self.config.num_disaster_types,
            embedding_dim=self.config.embedding_dim,
            model_channels=self.config.model_channels
        ).to(self.device)

        self.logger.info(f"âœ… Model created with {self._count_parameters(self.model):,} parameters")

        # åˆ›å»ºSAR VAE Decoder (å¯è®­ç»ƒ)
        self.logger.info("Creating SAR VAE Decoder...")
        self.sar_decoder = SARVAEDecoder(
            latent_channels=4,
            latent_size=64,
            output_channels=1,
            hidden_channels=128
        ).to(self.device)

        self.logger.info(f"âœ… SAR VAE Decoder created with {self._count_parameters(self.sar_decoder):,} parameters")

        # å†»ç»“ VAE Encoder (å¦‚æœæ¨¡å‹ä¸­æœ‰)
        try:
            # å¦‚æœä½¿ç”¨äº†é¢„è®­ç»ƒçš„VAE encoderï¼Œå†»ç»“å®ƒ
            for param in self.model.vae_encoder.parameters():
                param.requires_grad = False
            self.logger.info("âœ… VAE Encoder frozen")
        except AttributeError:
            self.logger.info("âš ï¸ No VAE encoder found to freeze")

        # åˆ›å»ºä¼˜åŒ–å™¨ - ä¼˜åŒ–å¯è®­ç»ƒçš„å‚æ•°
        trainable_params = list(filter(lambda p: p.requires_grad, self.model.parameters()))
        trainable_params.extend(self.sar_decoder.parameters())

        self.optimizer = optim.AdamW(
            trainable_params,
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )

        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=self.config.num_epochs,
            eta_min=1e-7
        )

        self.logger.info(f"âœ… Optimizer configured (lr={self.config.learning_rate})")

        if self.visualizer:
            self.visualizer.log_metrics(
                {
                    'stage': 'a',
                    'dataset_size': len(self.train_dataset),
                    'batch_size': self.config.batch_size,
                    'model_parameters': self._count_parameters(self.model)
                },
                step=0
            )

    def train_stage_a(self):
        """æ‰§è¡ŒStage Aè®­ç»ƒ"""
        self.setup_stage_a()

        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸš€ Starting Stage A Training")
        self.logger.info("=" * 80 + "\n")

        best_loss = float('inf')
        global_step = 0

        for epoch in range(self.config.num_epochs):
            epoch_loss = 0.0
            num_batches = 0

            self.model.train()

            for batch_idx, batch in enumerate(self.train_loader):
                global_step += 1

                # å‡†å¤‡æ•°æ®
                rgb = batch['rgb_image'].to(self.device)
                sar = batch['sar_image'].to(self.device)
                mask = batch.get('building_mask')
                if mask is not None:
                    mask = mask.to(self.device)

                disaster_type = batch.get('disaster_type')
                if disaster_type is not None:
                    disaster_type = disaster_type.to(self.device)

                disaster_severity = batch.get('disaster_severity')
                if disaster_severity is not None:
                    disaster_severity = disaster_severity.to(self.device)

                # å‰å‘ä¼ æ’­
                try:
                    # ControlNetç”Ÿæˆæ¡ä»¶
                    outputs = self.model(
                        sample=sar,
                        timestep=torch.randint(0, 1000, (rgb.size(0),)).to(self.device),
                        encoder_hidden_states=rgb,
                        rgb_image=rgb,
                        building_mask=mask,
                        disaster_type=disaster_type,
                        disaster_severity=disaster_severity
                    )

                    # SAR VAE Decoderè§£ç  (æ­¤å¤„outputsåº”è¯¥æ˜¯latentè¡¨ç¤º)
                    # å¦‚æœoutputsæ˜¯raw outputï¼Œéœ€è¦é€šè¿‡decoderç”ŸæˆSARå›¾åƒ
                    if hasattr(outputs, 'shape') and len(outputs.shape) == 4:
                        # å‡è®¾outputsæ˜¯(B, C, H, W)çš„latent
                        sar_pred = self.sar_decoder(outputs)
                    else:
                        sar_pred = outputs

                    # è®¡ç®—æŸå¤±
                    loss = F.mse_loss(sar_pred, sar)

                    # åå‘ä¼ æ’­
                    loss.backward()

                    if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                        self.optimizer.step()
                        self.optimizer.zero_grad()

                    # è®°å½•æŒ‡æ ‡
                    epoch_loss += loss.item()
                    num_batches += 1

                    self.metrics.update(loss=loss.item())

                    # å®šæœŸè®°å½•
                    if global_step % self.config.log_frequency == 0:
                        avg_loss = epoch_loss / num_batches
                        self.logger.info(
                            f"Epoch {epoch+1}/{self.config.num_epochs} | "
                            f"Batch {batch_idx+1}/{len(self.train_loader)} | "
                            f"Loss: {loss.item():.6f} | "
                            f"Avg Loss: {avg_loss:.6f}"
                        )

                        if self.visualizer:
                            self.visualizer.log_metrics(
                                {'loss': loss.item(), 'avg_loss': avg_loss},
                                step=global_step,
                                stage='stage_a'
                            )

                        # å¯è§†åŒ–ç‰¹å¾å’Œç»“æœ
                        if self.config.visualize_features and global_step % self.config.visualize_frequency == 0:
                            self.visualizer.log_training_comparison(
                                f'batch_{global_step}',
                                rgb=rgb[:1],
                                sar_pred=outputs[:1],
                                sar_gt=sar[:1],
                                mask=mask[:1] if mask is not None else None,
                                step=global_step,
                                stage='stage_a'
                            )

                except Exception as e:
                    self.logger.error(f"Error in batch {batch_idx}: {e}")
                    continue

            # Epochç»“æŸ
            epoch_loss /= num_batches
            self.scheduler.step()

            self.logger.info(
                f"\nâœ… Epoch {epoch+1}/{self.config.num_epochs} completed - Loss: {epoch_loss:.6f}\n"
            )

            if self.visualizer:
                self.visualizer.log_metrics(
                    {'epoch_loss': epoch_loss, 'lr': self.scheduler.get_last_lr()[0]},
                    step=epoch,
                    stage='stage_a'
                )

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config.save_frequency == 0:
                self._save_checkpoint(epoch, epoch_loss)

            # ä¿å­˜æœ€ä¼˜æ¨¡å‹
            if epoch_loss < best_loss:
                best_loss = epoch_loss
                self._save_checkpoint(epoch, epoch_loss, is_best=True)

        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸ‰ Stage A Training Completed!")
        self.logger.info(f"Best Loss: {best_loss:.6f}")
        self.logger.info("=" * 80)

        if self.visualizer:
            self.visualizer.finish()

    def _save_checkpoint(self, epoch: int, loss: float, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'config': self.config.to_dict()
        }

        if is_best:
            path = f"{self.config.checkpoint_dir}/best_model.pt"
            self.logger.info(f"ğŸ’¾ Saving best model (loss={loss:.6f}) to {path}")
        else:
            path = f"{self.config.checkpoint_dir}/checkpoint_epoch_{epoch+1:03d}.pt"
            self.logger.info(f"ğŸ’¾ Saving checkpoint to {path}")

        torch.save(checkpoint, path)

    def _count_parameters(self, model: nn.Module) -> int:
        """è®¡ç®—å‚æ•°æ•°é‡"""
        return sum(p.numel() for p in model.parameters() if p.requires_grad)


def main():
    parser = argparse.ArgumentParser(description='CONDSAR Training Script')

    # é…ç½®æ–‡ä»¶å‚æ•° (æ–°å¢)
    parser.add_argument('--config', type=str, default=None,
                       help='Configuration file path (.yaml or .json)')

    # åŸºç¡€å‚æ•°
    parser.add_argument('--stage', type=str, default='a', choices=['a', 'b', 'c'],
                       help='Training stage (a/b/c)')
    parser.add_argument('--source-dir', type=str, default=None,
                       help='Source domain dataset directory')
    parser.add_argument('--target-dir', type=str, default=None,
                       help='Target domain dataset directory')
    parser.add_argument('--batch-size', type=int, default=None,
                       help='Batch size')
    parser.add_argument('--num-epochs', type=int, default=None,
                       help='Number of epochs')
    parser.add_argument('--learning-rate', type=float, default=None,
                       help='Learning rate')
    parser.add_argument('--device', type=str, default=None,
                       help='Device (cuda/cpu)')
    parser.add_argument('--use-wandb', action='store_true', default=None,
                       help='Use WandB for logging')
    parser.add_argument('--wandb-offline', action='store_true',
                       help='Run WandB in offline mode')
    parser.add_argument('--output-dir', type=str, default=None,
                       help='Output directory')
    parser.add_argument('--run-name', type=str, default=None,
                       help='WandB run name')

    args = parser.parse_args()

    # ========== é…ç½®åŠ è½½æµç¨‹ ==========
    config_dict = {}

    # Step 1: å¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼ŒåŠ è½½é…ç½®æ–‡ä»¶
    if args.config:
        config_dict = load_config_file(args.config)

    # Step 2: åˆå¹¶é…ç½®å’Œå‘½ä»¤è¡Œå‚æ•° (å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æœ€é«˜)
    merged_config = merge_config_with_args(config_dict, args)

    # Step 3: ä½¿ç”¨åˆå¹¶åçš„é…ç½®åˆ›å»º TrainingConfig
    config = TrainingConfig(
        stage=merged_config.get('stage', 'a'),
        source_dir=merged_config.get('source_dir', './data'),
        target_dir=merged_config.get('target_dir', './data'),
        batch_size=merged_config.get('batch_size', 4),
        num_epochs=merged_config.get('num_epochs', 100),
        learning_rate=merged_config.get('learning_rate', 1e-4),
        device=merged_config.get('device', 'cuda'),
        use_wandb=merged_config.get('use_wandb', True),
        wandb_offline=merged_config.get('wandb_offline', False),
        output_dir=merged_config.get('output_dir', './outputs'),
        run_name=merged_config.get('run_name', None)
    )

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CondsarTrainer(config)

    # æ‰§è¡Œè®­ç»ƒ
    if config.stage == 'a':
        trainer.train_stage_a()
    else:
        print(f"Stage {config.stage} training not yet implemented")


if __name__ == '__main__':
    main()

